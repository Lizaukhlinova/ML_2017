{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Знакомство с линейным классификатором\n",
    "\n",
    "1. Как выглядит бинарный линейный классификатор? (Формула для отображения из множества объектов в множество классов.)\n",
    "\n",
    "    \\begin{equation*}\n",
    "    a(x) = \n",
    "     \\begin{cases}\n",
    "      1, &< w, x > + w_0 \\ge 0 \\\\\n",
    "      -1, &< w, x > + w_0 < 0\n",
    "     \\end{cases}\n",
    "    \\end{equation*}\n",
    "\n",
    "2. Что такое отступ алгоритма на объекте? Какие выводы можно сделать из знака отступа? \n",
    "\n",
    "    Отступом на объекте $x_i$ называется величина $M_i = y_i(<w, x_i> - w_0)$. Если отступ неположительный, то линейный классификатор выдал неправильный ответ на объекте $x_i$.\n",
    "\n",
    "3. Как классификаторы вида a(x) = sign(< w,x > − w0) сводят к классификаторам вида a(x) = sign(< w, x >)?\n",
    "\n",
    "    Можно включить $w_0$ в $w$, а в $x$ добавить еще одну координату, равную -1.\n",
    "\n",
    "4. Как выглядит запись функционала эмпирического риска через отступы? Какое значение он должен принимать для «наилучшего» алгоритма классификации?\n",
    "\n",
    "    Функционал эмпирического риска: $Q(w) = \\sum_{i=1}^l\\{M_i(w) < 0\\}$. Согласно пункту 2 наилучшее значения $Q = 0$ (когда нет ошибок классификации).\n",
    "\n",
    "5. Если в функционале эмпирического риска (риск с пороговой функцией потерь) всюду написаны строгие неравенства (Mi < 0) можете ли вы сразу придумать параметр w для алгоритма классификации a(x) = sign(< w, x >), минимизирующий такой функционал?\n",
    "\n",
    "    Подойдет $w = 0$. Тогда $a(x) = 0 => M_i = 0$\n",
    "\n",
    "6. Запишите функционал аппроксимированного эмпирического риска, если выбрана функция потерь L(M).\n",
    "\n",
    "    Функционал аппроксимированного эмпирического риска: $\\hat{Q}(w) = \\sum_{i=1}^l{L(M_i(w))}$\n",
    "\n",
    "7. Что такое функция потерь, зачем она нужна? Как обычно выглядит ее график?\n",
    "\n",
    "    Функция потерь - это неотрицательная функция, характеризующая величину ошибки классификатора (или др. алгоритма) a(x) на объекте x. Если функция потерь равна нулю, то класс угадан правильно. Через функцию потерь выражается функционал эмпирического риска Q, который отвечает за ошибку алгоритма на заданной выборке. Чем Q меньше, тем лучше. Обычно функции потерь гладкие, монотонные, убывают к нулю. \n",
    "\n",
    "8. Приведите пример негладкой функции потерь.\n",
    "\n",
    "    $f(x) = (1 - M)_+$\n",
    "\n",
    "9. Что такое регуляризация? Какие регуляризаторы вы знаете?\n",
    "\n",
    "    Регуляризация - способ уменьшить переобучение модели путем добавления штрафов на веса. Таким образом мы сбалансируем слишком большие веса. Самые частые регуляризации - l1 ($\\gamma \\sum_k{|w_k|}$) и l2 ($\\gamma \\sum_k{|w_k|^2}$). \n",
    "\n",
    "10. Как связаны переобучение и обобщающая способность алгоритма? Как влияет регуляризация на обобщающую способность?\n",
    "\n",
    "    Если модель склонна к переобучению, то ошибка на тестовой выборке будет больше ошибки на обучающей выборке. Обобщающая способность алгоритма предполагает, что ошибка будет примерно одинаковой на всех выборках. Регуляризация уменьшает переобучение, следовательно, увеличивает обобщающую способность алгоритма.\n",
    "\n",
    "11. Как связаны острые минимумы функционала аппроксимированного эмпирического риска с проблемой переобучения?\n",
    "\n",
    "    В этих точках ошибка очень большая, значит, скорее всего, модель будет сильно переобучаться.\n",
    "\n",
    "12. Что делает регуляризация с аппроксимированным риском как функцией параметров алгоритма?\n",
    "\n",
    "    Регуляризация уменьшает набор параметров, обнуляя набор весов. \n",
    "\n",
    "13. Для какого алгоритма классификации функционал аппроксимированного риска будет принимать большее значение на обучающей выборке: для построенного с регуляризацией или без нее? Почему?\n",
    "\n",
    "    Функционал аппроксимированного риска будет принимать большее значение на обучающей выборке для алгоритма, построенного с регуляризацией. Зато он не будет сильно переобучаться. Регуляризация уменьшает переобучение, но увеличивает ошибку.\n",
    "\n",
    "14. Для какого алгоритма классификации функционал риска будет принимать большее значение на тестовой выборке: для построенного с оправдывающей себя регуляризацией или вообще без нее? Почему?\n",
    "\n",
    "    В этом случае, наверное, функционал будет меньше на модели с регуляризацией.\n",
    "\n",
    "15-17. См. пункт 3.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Вероятностный смысл регуляризаторов\n",
    "\n",
    "Рассмотрим задачу линейной классификации с регуляризацией:\n",
    "\n",
    "$\\sum_{i=1}^lL(y_i, f(x_i)) + \\gamma V(w) \\rightarrow min$\n",
    "\n",
    "Введем обозначения:\n",
    "\n",
    "$p(x, y|w)$ - вероятностная модель данных,\n",
    "\n",
    "$p(w, C)$ - априорное распределение параметров модели\n",
    "\n",
    "Тогда принцип максимального правдопадобия выглядит следующим образом:\n",
    "\n",
    "$\\Pi = \\sum_{i = 1}^l \\ln(x_i,y_i|w)p(w, C) = \\sum_{i = 1}^l \\ln(x_i,y_i|w) + \\ln(p(w, C)) \\rightarrow max$\n",
    "\n",
    "Отсюда видно, что $\\ln(p(w, C))$ отвечает в этой оптимизации за регуляризацию.\n",
    "\n",
    "Для l1 и l2 регуляризации имеем:\n",
    "\n",
    "1. l1: $\\sum_{i=1}^lL(y_i, f(x_i)) + \\gamma \\sum_{k=1}^m|w_k| \\rightarrow min$\n",
    "\n",
    "    Пусть $w$ имеет n-мерное распределение Лапласса:\n",
    "\n",
    "    $p(w, C) = \\frac{1}{(2C)^n}exp(-\\frac{\\sum_{k=1}^m|w_k|}{C})$\n",
    "\n",
    "2. l2: $\\sum_{i=1}^lL(y_i, f(x_i)) + \\gamma \\sum_{k=1}^m|w_k|^2 \\rightarrow min$\n",
    "\n",
    "    Пусть $w$ имеет n-мерное гауссовское распределение:\n",
    "    \n",
    "    $p(w, C) = \\frac{1}{(2 \\pi C)^{0.5n}}exp(-\\frac{\\sum_{k=1}^m|w_k|^2}{2C})$\n",
    "    \n",
    "Логарифмируя, получим то, что нужно. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SVM и максимизация разделяющей полосы\n",
    "\n",
    "Рассмотрим случая, когда у нас классы линейно разделимы. Пусть $y = \\{-1; 1\\}$, $a(x) = sign(<w, x> - w_0)$. Задача состоит в том, чтобы расстояние от этой плоскости до объектов каждого из классов была максимальной.\n",
    "\n",
    "Подберем такие $w, w_0$, чтобы минимальный отступ был единицей $min_i {y_i(<x, x_i> - w_0)} = 1$. Пусть $x_+$ и $x_-$ - ближайшие к плоскости точки классов 1 и -1 соответственно. Тогда проекция вектора, соединяющего эти точки, на перпендекуляр к плоскости (ширина полосы):\n",
    "\n",
    "$<x_+ - x_-, \\frac{w}{\\|w\\|}> = \\frac{<w, x_+> - <w, x_->}{\\|w\\|} = \\frac{(w_0 + 1) - (w_0 - 1)}{\\|w\\|} = \\frac{2}{\\|w\\|} = $\n",
    "\n",
    "Задача максимизации ширины полосы ставится следующим образом:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\|w\\| \\rightarrow min\n",
    "\\\\\n",
    "y_i(<x, x_i> - w_0) \\ge 1, i = 1, \\dots, l.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Для неразделимой модели необходимо ослабить условия оптимизации, добавив штраф на попадение в разделяющую полосу.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "0.5\\|w\\|^2 + C\\sum_{i=1}^l \\xi_i \\rightarrow min\n",
    "\\\\\n",
    "y_i(<x, x_i> - w_0) \\ge 1 - \\xi_i, i = 1, \\dots, l,\n",
    "\\\\\n",
    "\\xi_i \\ge 0, i = 1,\\dots,l.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Объединим условия на $\\xi_i$, т.к. из системы видно, что $\\xi_i = max\\{0, 1 - y_i(<x, x_i> - w_0)\\} =: (1 - M_i)_+$\n",
    "\n",
    "Окончательно имеем:\n",
    "\n",
    "$\\sum_{i=1}^l(1 - M_i)_+ \\frac{1}{2C}\\|w\\|^2 \\rightarrow min$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Kernel trick\n",
    "\n",
    "Придумайте ядро, которое позволит линейному классификатору с помощью Kernel Trick построить в исходном пространстве признаков разделяющую поверхность $x^2_1 + 2x^2_2 = 3$. Какой будет размерность спрямляющего пространства?\n",
    "\n",
    "Подберем функцию $K(w,x) = <\\phi(w), \\phi(x)>$ в виде:\n",
    "\n",
    "$K(w,x) = (w_1x_1 + x_2x_2)^2 = w_1^2x_1^2 + 2w_1x_1w_2x_2 + w_2^2x_2^2 = (w_1^2, \\sqrt{2}w_1w_2, w_2^2)\\begin{pmatrix}x_1^2\\\\\\sqrt{2}x_1x_2\\\\x_2^2\\end{pmatrix}$\n",
    "\n",
    "Таким образом, преобразование $K$ сопоставляет вектору $(w1, w2)$ из пространства размерности 2 вектор $(w_1^2, \\sqrt{2}w_1w_2, w_2^2)$ пространства размерности 3 (размерность стремляющего пространства). Тогда поверхнотсь будет задаваться в этом пространстве уравнением:\n",
    "\n",
    "$(w_1^2, \\sqrt{2}w_1w_2, w_2^2)\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix} + x_0 = 0$\n",
    "\n",
    "В нашем случае $x_0 = -3, x_1 = 1, x_2 = 0, x_3 = 2$ задает разделяющую поверхность в трехмерном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 l1-регуляризация\n",
    "\n",
    "__Теорема Куна-Таккера:__\n",
    "\n",
    "Для задачи оптимизации:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "f(x) \\rightarrow min_x\n",
    "\\\\\n",
    "g(x) \\le 0.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "точка $x$ является решением, тогда и только тогда, когда существуют такое $\\mu$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\nabla_xL(x,\\mu) = \\nabla_x(f(x)+\\mu g(x)) = 0, \n",
    "\\\\\n",
    "\\mu g(x) = 0,\n",
    "\\\\\n",
    "\\mu \\ge 0.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Рассмотрим нашу задачу. Пусть у нас есть условие ограничения l1-нормы вектора весов числом.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\sum L \\rightarrow min_x,\n",
    "\\\\\n",
    "\\sum_{i=1}^n|w_i| \\le \\alpha\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Введем лагранжиан $M = \\sum L + \\mu (\\sum_{i=1}^n|w_i| - \\alpha)$. Тогда по теореме Куна-Таккера эта задача эквивалента:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\nabla_xM = 0, \n",
    "\\\\\n",
    "\\mu = 0 \\text{ или } \\sum_{i=1}^n|w_i| = \\alpha,\n",
    "\\\\\n",
    "\\mu \\ge 0.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "то есть добавлению штрафа с l1 нормой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Повторение: метрики качества\n",
    "\n",
    "1. Что представляют собой метрики качества Accuracy, Precision и Recall?\n",
    "    * Accuracy - доля правильных ответов при классификации.\n",
    "    \n",
    "    * Precision - мера точности ответов. Она определяет, сколько из положительных ответов классификатора  совпадают с правильным ответами.\n",
    "    \n",
    "    * Recall - мера полноты ответов. Она определяет, сколько из правильных положительных ответов были угаданы классификатором.\n",
    "    \n",
    "2. Что такое метрика качества AUC и ROC-кривая?\n",
    "\n",
    "    ROC-кривая - график зависимости TPR (true positive rate) от FPR (false positive rate). \n",
    "    \n",
    "    Введем обозначения:\n",
    "    \n",
    "    * tp - угадали положительный ответ\n",
    "    * tn - не угадали положительный ответ\n",
    "    * fp - не угадали отрицательный ответ\n",
    "    * fn - угадали отрицательный ответ\n",
    "    \n",
    "  Тогда TPR = $\\frac{tp}{tp + fn}$ - доля правильных положительных ответов ко всем правильным ответам, а FPR = $\\frac{fp}{fp + tn}$ - доля отрицательных ответов, которые неверно предсказаны как положительные, ко всем отрицательным ответам. Чем больше TPR, тем меньше положительных ответов мы упцстим, чем больше FPR - тем меньше отрицательныхю \n",
    "\n",
    "    ROC кривая обязательно проходит через точки 0 и 1. AUC - площадь под графиком ROC-кривой.\n",
    "\n",
    "3. Как построить ROC-кривую (нужен алгоритм), если например, у вас есть правильные ответы к домашнему заданию про фамилии и ваши прогнозы?\n",
    "\n",
    "    В задаче на фамилии можно преобразовать y к виду {1; -1} и построить линеунцю модель  $a(x) = sign(< w,x > −w0)$. Тогда будем изменять $w_0$ от 0 до 1 и для каждого значения считать TPR и FPR по формулам из предыдущего пункта. Получим точки графика.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
